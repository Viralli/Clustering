K Means Clustering

Definition

K Means Clustering is a widely used unsupervised machine learning algorithm that divides a dataset into K distinct, non-overlapping clusters. It iteratively assigns each data point to one of the K clusters based on 
feature similarity.

K Means Clustering with Convex Hulls Visualization

Overview

This project illustrates the application of the K Means algorithm to group a sample dataset and visualize the clusters with convex hulls. The convex hulls outline each cluster's boundaries, giving a clear geometric view of 
the data points and making the clustering results easier to understand.

Convex Hulls

Convex hulls are geometric shapes that outline the outer boundaries of each cluster. They provide a clear representation of how data points are grouped together in space.

Implementation Details

Dataset Generation: We generate a synthetic dataset using `make_blobs` with 300 data points, 4 centers, and a standard deviation of 0.60.

K Means Clustering: The algorithm is initialized with K=4 clusters using the `KMeans` class from `sklearn.cluster`.

Visualization:

Part 1: Displays a scatter plot of the clustered data points, where each cluster is assigned a distinct color.

Part 2: Visualizes the convex hulls of the clusters using plot_convex_hulls. Each convex hull outlines the boundary of its corresponding cluster, offering a visual understanding of cluster shapes and distributions.

Benefits of Convex Hulls Visualization

Geometric Insight: Convex hulls provide geometric insights into the structure of clusters, helping in understanding the spatial arrangement and boundaries of data groups.

Enhanced Interpretation: By adding convex hulls to the scatter plot, we improve the clarity of the clustering results, making it easier to see how data points are grouped into clusters.

Convergence of K Means Clustering

The K Means algorithm converges when the centroids of the clusters no longer change significantly between iterations or when a specified number of iterations is reached. Convergence is achieved when the assignments of 
data points to clusters stabilize and centroids settle into their final positions.

Time Complexity in K Means Clustering

The time complexity of the K Means algorithm is O(n * K * I * d), where:

n is the number of data points,
K is the number of clusters,
I is the number of iterations until convergence,
d is the number of features in each data point.

K-Means Clustering Algorithm Overview

The K-Means algorithm is a widely used unsupervised learning method for partitioning data into k clusters based on similarity. It iteratively refines the position of centroids to 
minimize the variance within each cluster, aiming to find cluster centers that best represent the data.

Key Concepts:

1. Initialization:

K-Means begins by randomly initializing k centroids in the feature space. These centroids represent the initial guesses of where the cluster centers might be.

2. Assignment Step:

Each data point is assigned to the nearest centroid based on a distance metric, commonly the Euclidean distance. This step aims to minimize the distance between each point and its 
assigned centroid.

3. Update Step:

After all points have been assigned to clusters, the centroids are updated by computing the mean of all data points assigned to each cluster. This new centroid becomes the center of its 
respective cluster.

4. Convergence:

Steps 2 and 3 are repeated iteratively until the centroids no longer significantly change between iterations, or a specified number of iterations is reached. This indicates that the
algorithm has converged.

Algorithm Steps:

Step 1: Initialization

    Initialize k centroids randomly within the feature space.

Step 2: Assignment

   For each data point, calculate its distance to each centroid using a distance metric (e.g., Euclidean distance).

   Assign the data point to the cluster represented by the nearest centroid

Step 3: Update

  Once all data points are assigned, update each centroid by computing the mean of all points assigned to that cluster.

  This repositions the centroid to the center of its cluster in the feature space.

Step 4: Iteration

  Repeat steps 2 and 3 until convergence criteria are met (e.g., centroids do not change significantly, or maximum iterations reached).

Advantages:

  Scalability: K-Means is computationally efficient and can handle large datasets with relatively low complexity.

  Ease of Implementation: It is straightforward to understand and implement, making it a popular choice for clustering tasks.

  Versatility: K-Means can be applied to various types of data and is robust in finding clusters with spherical shapes and similar sizes.

Limitations:

   Centroid Initialization: Results can vary based on initial centroid placement, affecting cluster quality.

  Cluster Shape Assumption: K-Means assumes clusters are spherical and of similar density, which may not always reflect real-world data.

Use Cases:

    Customer Segmentation: Grouping customers based on purchasing behavior or demographics.

    Image Segmentation: Identifying regions of interest in images based on pixel similarity.

    Anomaly Detection: Detecting outliers or unusual patterns in data points.

The K-Means algorithm iteratively partitions data into k clusters by minimizing the variance within each cluster. By sequentially assigning data points to the nearest centroid and
updating centroids based on the mean of assigned points, K-Means efficiently identifies clusters in the data space. Its simplicity and effectiveness make it a valuable tool in 
exploratory data analysis and pattern recognition tasks.

The following section illustrates an implementation example of the K-Means Algorithm: (K Means Clustering Algorithm Example)

Initial Setup:

we are assuming the dataset,

Dataset:
                Points=[(1,2),(1.5,1.8),(5,8),(8,8),(1,0.6),(9,11)]

We are assuming the number of clusters, 

Number of clusters (K): 2

The initial centroids in the K-means algorithm are chosen randomly from the data points:

Initial centroids:

Centroid C1 = (1,2)
Centroid C2 = (5,8)

Step-by-Step Process:

1. Assign Data Points to Nearest Centroid 

Point (1, 2):

Distance to C1: Square root of (1−1)^2 + (2−2)^2 = Square root of 0 = 0

Distance to C2: Square root of (1−5)^2 + (2−8)^2 = Square root of (−4)^2 +(−6)^2 = Square root of 16+36 = Square root of 52≈7.21

Since the distance to C1(0) is less than the distance to C2(7.21), point(1,2) is assigned to Cluster 1.

Point (1.5, 1.8):

Distance to C1:

Square root of (1.5−1)^2+(1.8−2)^2 = Square root of (0.5)^2+(−0.2)^2 = Square root of 0.25+0.04 = Square root of 0.29≈0.54

Distance to C2:

Square root of (1.5−5)^2 +(1.8−8)^2 = Square root of (−3.5)^2 +(−6.2)^2 = Square root of 12.25+38.44 = Square root of 50.69≈7.12

Since the distance to C1(0.54) is less than the distance to C2(7.12), point (1.5, 1.8) is assigned to Cluster 1.

Point (5, 8):

Distance to C1:

Square root of (5−1)^2+(8−2)^2 = Square root of 4^2+6^2 = Square root of 16 + 36 = Square root of 52≈7.21

Distance to C2:

Square root of (5−5)^2+(8−8)^2=Square root of 0^2 + 0^2 = Square root of 0+0 = Square root of 0 = 0

Since the distance to C2(0) is less than the distance to C1(7.21), point (5, 8) is assigned to Cluster 2.

Point (8, 8):

Distance to C1:

Square root of (8−1)^2 +(8−2)^2 = Square root of 7^2 +6^2 = Square root of 49+36 = Square root of 85≈9.22

Distance to C2:

Square root of (8−5)^2 +(8−8)^2 = Square root of 3^2+0^2 = Square root of 9+0 = Square root of 9=3

Since the distance to C2(3) is less than the distance to C(1) (9.22), point (8, 8) is assigned to Cluster 2.

Point (1, 0.6):

Distance to C1:

Square root of (1−1)^2+(0.6−2)^2=Square root of 0^2 +(−1.4)^2 = Square root of 0+1.96 = Square root of 1.96≈1.4

Distance to C2:

Square root of (1−5)^2+(0.6−8)^2=Square root of (−4)^2+(−7.4)^2 = Square root of 16+54.76 = Square root of 70.76≈8.41

Since the distance to C1 (1.4) is less than the distance to C2(8.41), point (1, 0.6) is assigned to Cluster 1.

Point (9, 11):

Distance to C1:

Square root of (9−1)^2 +(11−2)^2=Square root of 8^2 +9^2 = Square root of 64+81=Square root of 145≈12.04

Distance to C2:

Square root of (9−5)^2 +(11−8)^2 =Square root of 4^2+3^2 = Square root of 16+9 = Square root of 25=5

Since the distance to C2(5) is less than the distance to C1(12.04), point (9, 11) is assigned to Cluster 2.

2. Update Centroids

Now, we recalculate the centroids based on the current cluster assignments.

Cluster 1:

Points assigned: (1, 2), (1.5, 1.8), (1, 0.6)
New centroid C1: (1+1.5+1)/3, (2+1.8+0.6)/3 = (3.5/3,4.4/3) ≈(1.17,1.47)

Cluster 2:

Points assigned: (5, 8), (8, 8), (9, 11)
New centroid C2: (5+8+9)/3, (8+8+11)/3 = (22/3, 27/3) = (7.33,9)

3. Repeat Steps 1 and 2

We repeat the process of assigning points to the nearest centroid and updating the centroids until the centroids no longer change significantly.

For example, let's go through one more iteration.

Reassign Data Points:

Using the updated centroids C1 = (1.17,1.47) and C2 = (7.33,9), we calculate the distances again and reassign the points.

Point (1, 2):

Distance to C1: Square root of (1−1.17)^2 +(2−1.47)^2 = Square root of (−0.17)^2 + (0.53)^2 = Square root of 0.0289+0.2809 = Square root of 0.3098≈0.54

Distance to C2:

Square root of (1−7.33)^2 + (2−9)^2 = Square root of (−6.33)^2 +(−7)^2 = Square root of 40.0689+49 = Square root of 89.0689≈9.42

Assignment:

Point (1, 2) remains in Cluster 1.

Point (1.5, 1.8):

Distance to C1:

Square root of (1.5−1.17)^2 + (1.8−1.47)^2 = Square root of (0.33)^2 +(0.33)^2 = Square root of 0.1089+0.1089 = Square root of 0.2178≈0.47

Distance to C2:

Square root of (1.5−7.33)^2 +(1.8−9)^2 = Square root of (−5.83)^2 +(−7.2)^2 = Square root of 33.9889+51.84 = Square root of 85.8289≈9.14

Assignment:

Point (1.5, 1.8) remains in Cluster 1.

Point (5, 8):

Distance to C1:

Square root of (5−1.17)^2 +(8−1.47)^2 = Square root of (3.83)^2 +(6.53)^2 = Square root of 14.6689+42.7009 = Square root of 57.3698≈8.14

Distance to C2:

Square root of (5−7.33)^2 +(8−9)^2 = Square root of (−2.33)^2 +(−1)^2 = Square root of 5.4289+1 = Square root of 6.4289≈2.43

Assignment:

Point (5, 8) remains in Cluster 2.

Point (8, 8):

Distance to C1:

Square root of (8−1.17)^2+(8−1.47)^2 = Square root of (6.83)^2 +(6.53)^2 = Square root of 46.6489+42.7009 = Square root of 89.3498≈9.45

Distance to C2:

Square root of (8−7.33)^2 +(8−9)^2 = Square root of (0.67)^2 +(−1)^2 = Square root of 0.4489+1 = Square root of 1.4489≈1.03

Assignment:

Point (8, 8) remains in Cluster 2.

Point (1, 0.6):

Distance to C1:

Square root of (1−1.17)^2 +(0.6−1.47)^2 = Square root of (−0.17)^2 + (−0.87)^2 = Square root of 0.0289+0.7569 = Square root of 0.7858≈0.89

Distance to C2:

Square root of (1−7.33)^2 +(0.6−9)^2 = Square root of (−6.33)^2 +(−8.4)^2 = Square root of 40.0689+70.56 = Square root of 110.6289≈11.05

Assignment:

Point (1, 0.6) remains in Cluster 1.

Point (9, 11):

Distance to C1:

Square root of (9−1.17)^2 +(11−1.47)^2 = Square root of (7.83)^2 +(9.53)^2 = Square root of 61.3089+90.7609 = Square root of 152.0698≈13.2

Distance to C2:

Square root of (9−7.33) ^2 + (11−9)^2 = Square root of (1.67)^2 +(2)^2 = Square root of 2.7889+4 = Square root of 6.7889≈2.43

Assignment:

Point (9, 11) remains in Cluster 2.

Since the assignments did not change significantly, the process stops here.

Final Clusters

Cluster 1:

Points: (1, 2), (1.5, 1.8), (1, 0.6)

Cluster 2:

Points: (5, 8), (8, 8), (9, 11)

Final Centroids:

C1≈(1.17,1.47)

C2≈(7.33,9)

Conclusion:

The K-means algorithm has successfully partitioned the data into two clusters based on the Euclidean distance between points and centroids. Each point has been assigned to the cluster 
with the nearest centroid, and the centroids have been updated iteratively until they stabilized.

Basic Terms in K-Means Clustering:

1. Centroid:

    Definition: Centroids are the center points of clusters in K-Means clustering. They are initially chosen randomly or based on a rule of thumb and are iteratively updated to minimize 
    the distance between data points and centroids within each cluster.

    Usage: Centroids indicate the average location of data points within their clusters and act as reference points for assigning clusters and achieving convergence in the algorithm.    

2. Cluster:

    Definition: A cluster refers to a group of data points that are similar to each other based on a specified distance metric, such as Euclidean distance in K-Means. Data points within 
    the same cluster are more similar to each other compared to those in other clusters.

    Usage: Clusters in K-Means clustering are formed around centroids, where each data point is assigned to the cluster with the nearest centroid during the assignment step of the 
    algorithm.

3. Euclidean Distance:

    Definition: Euclidean distance is a measure of distance between two points in Euclidean space. In K-Means clustering, it is commonly used to calculate the similarity or dissimilarity 
    between data points and centroids.

    Usage: Euclidean distance determines how close a data point is to a centroid, influencing which cluster the data point will be assigned to during each iteration of the K-Means 
    algorithm.

4. Initialization:

    Definition: Initialization refers to the process of setting up the initial positions of centroids at the beginning of the K-Means clustering algorithm. This can impact the convergence 
    and quality of clustering results.

    Usage: Proper initialization methods ensure that centroids are well-placed within the data space, contributing to faster convergence and more accurate clustering outcomes in K-Means.

5. Convergence:

    Definition: Convergence in the K-Means algorithm refers to the point where centroids no longer change significantly between iterations, indicating that the algorithm has reached a 
    stable clustering solution.

    Usage: Monitoring convergence is crucial in K-Means to determine when to stop iterating. It is typically achieved when the positions of centroids stabilize and data points remain 
    consistently assigned to their respective clusters.

6. Iteration:

    Definition: Iteration refers to each cycle of the K-Means algorithm where centroids are recalculated based on current cluster assignments, and data points are reassigned to the 
    nearest centroids.

    Usage: The algorithm iterates until convergence criteria are met, adjusting centroids and updating cluster assignments iteratively to improve clustering accuracy.
